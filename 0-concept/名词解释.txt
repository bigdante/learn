denoising autoencoding降噪自动编码器
为了输入和输出通过中间层后仍然比较相似，用了autoencoding，然后为了防止过拟合，将输入信号中的某些置为0，达到去噪声的效果。

一、 自回归语言模型

听到自回归语言模型（Autoregressive LM）这个词，我们知道一般的语言模型都是从左到右计算某个词出现的概率，但是当我们做完型填空或者阅读理解这一类NLP任务的时候词的上下文信息都是需要考虑的，
而这个时候只考虑了该词的上文信息而没有考虑到下文信息。所以，反向的语言模型出现了，就是从右到左计算某个词出现的概率，这一类语言模型称之为自回归语言模型。
像坚持只用单向Transformer的GPT就是典型的自回归语言模型，也有像ELMo那种拼接两个上文和下文LSTM的变形自回归语言模型。

二、自编码语言模型

自编码语言模型（Autoencoder LM）区别于上一节所述，自回归语言模型是根据上文或者下文来预测后一个单词。
那不妨换个思路，我把句子中随机一个单词用[mask]替换掉，是不是就能同时根据该单词的上下文来预测该单词。
我们都知道Bert在预训练阶段使用[mask]标记对句子中15%的单词进行随机屏蔽，然后根据被mask单词的上下文来预测该单词，这就是自编码语言模型的典型应用。

三、两种模型的优缺点对比

自回归语言模型没能自然的同时获取单词的上下文信息（ELMo把两个方向的LSTM做concat是一个很好的尝试，但是效果并不是太好），
而自编码语言模型能很自然的把上下文信息融合到模型中（Bert中的每个Transformer都能看到整句话的所有单词，等价于双向语言模型），但自编码语言模型也有其缺点，就是在Fine-tune阶段，
模型是看不到[mask]标记的，所以这就会带来一定的误差。XLNet将二者的上述优缺点做了一个完美的结合，在自回归语言模型中自然地引入上下文信息，并且解决自编码语言模型两阶段保持一致的问题。