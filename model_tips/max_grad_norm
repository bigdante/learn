gradnorm是一种优化方法，在多任务学习（Multi-Task Learning）中，
解决
1. 不同任务loss梯度的量级（magnitude）不同，造成有的task在梯度反向传播中占主导地位，模型过分学习该任务而忽视其它任务；
2. 不同任务收敛速度不一致；

max_grad_norm是一种控制梯度的技术，通常用于梯度裁剪。
在训练深度学习模型时，可能会出现梯度爆炸（gradient explosion）或梯度消失（gradient vanishing）的问题，这会导致训练过程不稳定。
为了解决这个问题，可以使用梯度裁剪来控制梯度的范数，以使梯度的大小保持在一个合理的范围内。
max_grad_norm就是梯度裁剪的一个参数，它指定了梯度的最大范数。
在反向传播过程中，如果梯度的范数超过了max_grad_norm，那么就会将梯度按比例缩小，使其范数不超过max_grad_norm。
这样可以确保梯度不会变得太大，从而避免梯度爆炸的问题。